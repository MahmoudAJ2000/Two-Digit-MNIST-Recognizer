{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Two-Digit MNIST Recognizer.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOaz7D00zZSYXz+j/sYSJi0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahmoudAJ2000/Two-Digit-MNIST-Recognizer/blob/main/Two_Digit_MNIST_Recognizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWltbtVQ82r9",
        "outputId": "e8b03d36-c666-4e76-b284-5a3b37b8d967"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n22hf-Cg84pS"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwZjsZGo87NL"
      },
      "source": [
        "from collections import OrderedDict\n",
        "class _DenseLayer(nn.Sequential):\n",
        "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
        "        super(_DenseLayer, self).__init__()\n",
        "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
        "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
        "                        growth_rate, kernel_size=1, stride=1, bias=False)),\n",
        "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
        "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
        "                        kernel_size=3, stride=1, padding=1, bias=False)),\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        new_features = super(_DenseLayer, self).forward(x)\n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
        "        return torch.cat([x, new_features], 1)\n",
        "\n",
        "\n",
        "class _DenseBlock(nn.Sequential):\n",
        "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
        "        super(_DenseBlock, self).__init__()\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n",
        "            self.add_module('denselayer%d' % (i + 1), layer)\n",
        "\n",
        "\n",
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, num_input_features, num_output_features):\n",
        "        super(_Transition, self).__init__()\n",
        "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
        "        self.add_module('relu', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
        "                                          kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    r\"\"\"Densenet-BC model class, based on\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
        "        block_config (list of 4 ints) - how many layers in each pooling block\n",
        "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
        "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
        "          (i.e. bn_size * k features in the bottleneck layer)\n",
        "        drop_rate (float) - dropout rate after each dense layer\n",
        "        num_classes (int) - number of classification classes\n",
        "    \"\"\"\n",
        "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
        "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=55):\n",
        "\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        # First convolution\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv0', nn.Conv2d(1, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
        "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
        "            ('relu0', nn.ReLU(inplace=True)),\n",
        "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2,padding=1)),\n",
        "        ]))\n",
        "\n",
        "        # Each denseblock\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
        "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
        "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
        "                self.features.add_module('transition%d' % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
        "\n",
        "        # Linear layer\n",
        "        self.classifier = nn.Linear(2208, num_classes)\n",
        "        self.classifier2 = nn.Linear(2208, 37)\n",
        "        self.classifier3 = nn.Linear(2208, 37)\n",
        "        self.classifier4 = nn.Linear(2208, 37)\n",
        "        self.classifier5 = nn.Linear(2208, 37)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        out = F.relu(features, inplace=True)\n",
        "        out = F.avg_pool2d(out, kernel_size=2).view(features.size(0), -1)\n",
        "        out1 = self.classifier(out)\n",
        "        out2 = self.classifier2(out)\n",
        "        out3 = self.classifier3(out)\n",
        "        out4 = self.classifier4(out)\n",
        "        out5 = self.classifier5(out)\n",
        "        return out1,out2,out3,out4,out5"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PazgeHUB89RI",
        "outputId": "28ba12f0-1383-42b5-e9c2-e43f902d0a5e"
      },
      "source": [
        "\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "def classify_and_detect(images):\n",
        "    \"\"\"\n",
        "\n",
        "    :param np.ndarray images: N x 4096 array containing N 64x64 images flattened into vectors\n",
        "    :return: np.ndarray, np.ndarray\n",
        "    \"\"\"\n",
        "\n",
        "    N = images.shape[0]\n",
        "\n",
        "    model = DenseNet(num_init_features=96, growth_rate=48, block_config=(6, 12, 36, 24)).to(device)    \n",
        "    checkpoint = torch.load('/content/gdrive/My Drive/Colab Notebooks/checkpoints/ckptBestTestC+l_10step.pt')\n",
        "    model.load_state_dict(checkpoint)\n",
        "    #model = train_model().to(device)\n",
        "    pred_class,pred_bboxes = evaluate(images, model)\n",
        "\n",
        "    return pred_class, pred_bboxes\n",
        "\n",
        "config = {\n",
        "    'valid_num':5000,\n",
        "    'train_num':55000,\n",
        "    'batch_size': 125 , \n",
        "    'num_classes': 55, \n",
        "    'epochs': 1000,\n",
        "    'lr':0.1,\n",
        "    'step_size':10,\n",
        "    'weight_decay':0.0005,\n",
        "    'momentum':0.9,\n",
        "    'gamma':0.1\n",
        "}\n",
        "\n",
        "\n",
        "def load_training_data():\n",
        "  prefix = \"train\"\n",
        "  directory = \"/content/gdrive/My Drive/Colab Notebooks/MNISTDD_train_valid/\"\n",
        "  train_images = np.load(directory+ prefix + \"_X.npy\")\n",
        "  train_labels = np.load(directory+ prefix + \"_Y.npy\")\n",
        "  train_bboxes = np.load(directory+ prefix + \"_bboxes.npy\")\n",
        "  prefix = \"valid\"\n",
        "  valid_images = np.load(directory+ prefix + \"_X.npy\")\n",
        "  valid_labels = np.load(directory+ prefix + \"_Y.npy\")\n",
        "  valid_bboxes = np.load(directory+ prefix + \"_bboxes.npy\")\n",
        "  return train_images, train_labels, train_bboxes, valid_images,valid_labels,valid_bboxes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    print(\"Training...\")\n",
        "\n",
        "    model = DenseNet(num_init_features=96, growth_rate=48, block_config=(6, 12, 36, 24)).to(device)\n",
        "    train_images, train_labels, train_bboxes, valid_images,valid_labels,valid_bboxes = load_training_data()\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    params = model.parameters()\n",
        "    optimizer = torch.optim.SGD(model.parameters(),config['lr'],\n",
        "                                momentum=config['momentum'],\n",
        "                                nesterov=True,\n",
        "                                weight_decay=config['weight_decay'])\n",
        "    # checkpoint = torch.load('/content/gdrive/My Drive/Colab Notebooks/checkpoints/ckptBestTestC+l_3.pt')\n",
        "    # model.load_state_dict(checkpoint)\n",
        "    # vars = torch.load('/content/gdrive/My Drive/Colab Notebooks/checkpoints/ckptBestDenseTrainC+l_6.pt')\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=config['step_size'], gamma=config['gamma'])# In your for loop\n",
        "    # losses_epochs = [vars['loss']]\n",
        "    # accs_epochs = [vars['acc']]\n",
        "    # iou_epochs = [vars['iou']]\n",
        "    losses_epochs = []\n",
        "    accs_epochs = []\n",
        "    iou_epochs = []\n",
        "    for epoch in range(config['epochs']):\n",
        "      model.train()\n",
        "      losses = []\n",
        "      index_start = 0\n",
        "      for batch_idx in range(config['train_num']//config['batch_size']):\n",
        "        optimizer.zero_grad()\n",
        "        index_end = min(index_start+config['batch_size'],config['train_num'])\n",
        "        train_images_tensor = torch.tensor(train_images[index_start:index_end],dtype = torch.float32).to(device)\n",
        "        train_labels_double_digits = convert_to_labels(train_labels,config['train_num'])\n",
        "        train_labels_l1_one_hot = np.eye(config['num_classes'])[train_labels_double_digits]\n",
        "        train_labels_l2_one_hot = np.eye(37)[train_bboxes[index_start:index_end][:,0][:,0]]\n",
        "        train_labels_l3_one_hot = np.eye(37)[train_bboxes[index_start:index_end][:,0][:,1]]\n",
        "        train_labels_l4_one_hot = np.eye(37)[train_bboxes[index_start:index_end][:,1][:,0]]\n",
        "        train_labels_l5_one_hot = np.eye(37)[train_bboxes[index_start:index_end][:,1][:,1]]\n",
        "        train_labels_tensor_l1 = torch.tensor(train_labels_l1_one_hot[index_start:index_end],dtype = torch.float32).to(device)\n",
        "        train_labels_tensor_l2 = torch.tensor(train_labels_l2_one_hot,dtype = torch.float32).to(device)\n",
        "        train_labels_tensor_l3 = torch.tensor(train_labels_l3_one_hot,dtype = torch.float32).to(device)\n",
        "        train_labels_tensor_l4 = torch.tensor(train_labels_l4_one_hot,dtype = torch.float32).to(device)\n",
        "        train_labels_tensor_l5 = torch.tensor(train_labels_l5_one_hot,dtype = torch.float32).to(device)\n",
        "        index_start = index_end\n",
        "        train_images_tensor=train_images_tensor.view(-1,1,64,64)\n",
        "        l1,l2,l3,l4,l5 = model(train_images_tensor)\n",
        "        loss1 = criterion(l1, train_labels_tensor_l1)\n",
        "        loss2 = criterion(l2, train_labels_tensor_l2)\n",
        "        loss3 = criterion(l3, train_labels_tensor_l3)\n",
        "        loss4 = criterion(l4, train_labels_tensor_l4)\n",
        "        loss5 = criterion(l5, train_labels_tensor_l5)\n",
        "        loss = loss1+loss2+loss3+loss4+loss5\n",
        "        loss.backward()\n",
        "        optimizer.step()  # update the weights\n",
        "        losses.append(loss.item())\n",
        "        if batch_idx%5 ==0:\n",
        "          print(\"Train Epoch: {} batch {} done, loss {}\".format(epoch,batch_idx,loss.item()))\n",
        "      avg_loss = sum(losses)/len(losses)\n",
        "      losses_epochs.append(avg_loss)\n",
        "      acc,iou = validate(valid_images,valid_labels,valid_bboxes,model)\n",
        "      accs_epochs.append(acc)\n",
        "      iou_epochs.append(round(iou,4))\n",
        "      print('Validation: Train Epoch: {}\\tLoss: {:.6f}\\t Accuracy: {}\\tiou: {}'.format(epoch, avg_loss,acc,round(iou,4)))\n",
        "      if max(accs_epochs) == acc and max(iou_epochs) == round(iou,4):\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss':loss,\n",
        "            'acc':acc,\n",
        "            'iou':iou\n",
        "            }, '/content/gdrive/My Drive/Colab Notebooks/checkpoints/ckptBestDenseTrainC+l_10step.pt')\n",
        "        torch.save(model.state_dict(), '/content/gdrive/My Drive/Colab Notebooks/checkpoints/ckptBestTestC+l_10step.pt')\n",
        "        torch.save(model.to(torch.device(device)), '/content/gdrive/My Drive/Colab Notebooks/models/modelDenseBestC+l_10step.pt')\n",
        "        print(\"Checkpoint saved\")\n",
        "      lr_scheduler.step()  # learnig rate annealing\n",
        "\n",
        "    return model\n",
        "\n",
        "def validate(valid_images,valid_labels,valid_bboxes,model):\n",
        "    print(\"Validating...\")\n",
        "    model.eval()\n",
        "    l1_outputs = []\n",
        "    l2_outputs = []\n",
        "    l3_outputs = []\n",
        "    l4_outputs = []\n",
        "    l5_outputs = []\n",
        "    with torch.no_grad():\n",
        "      index_start = 0\n",
        "      for batch_idx in range(valid_images.shape[0]//config['batch_size']):\n",
        "        index_end = min(index_start+config['batch_size'],valid_images.shape[0])\n",
        "        images_tensor = torch.tensor(valid_images[index_start:index_end],dtype = torch.float32).to(device)\n",
        "        images_tensor = images_tensor.view(-1,1,64,64)\n",
        "        l1,l2,l3,l4,l5 = model(images_tensor)\n",
        "        l1_outputs.append(l1)\n",
        "        l2_outputs.append(l2)\n",
        "        l3_outputs.append(l3)\n",
        "        l4_outputs.append(l4)\n",
        "        l5_outputs.append(l5)\n",
        "        index_start = index_end\n",
        "    l1_output = torch.cat(l1_outputs, dim=0)\n",
        "    l2_output = torch.cat(l2_outputs, dim=0)\n",
        "    l3_output = torch.cat(l3_outputs, dim=0)\n",
        "    l4_output = torch.cat(l4_outputs, dim=0)\n",
        "    l5_output = torch.cat(l5_outputs, dim=0)\n",
        "    l1_np = l1_output.cpu().detach().numpy()\n",
        "    l2_np = l2_output.cpu().detach().numpy()\n",
        "    l3_np = l3_output.cpu().detach().numpy()\n",
        "    l4_np = l4_output.cpu().detach().numpy()\n",
        "    l5_np = l5_output.cpu().detach().numpy()\n",
        "    classes = convert_from_labels(l1_np,valid_images.shape[0])\n",
        "    l2_og = convert_one_hot_to_original_format(l2_np,valid_images.shape[0])\n",
        "    l3_og = convert_one_hot_to_original_format(l3_np,valid_images.shape[0])\n",
        "    l4_og = convert_one_hot_to_original_format(l4_np,valid_images.shape[0])\n",
        "    l5_og = convert_one_hot_to_original_format(l5_np,valid_images.shape[0])\n",
        "    boxes = convert_boxes_to_original_format(l2_og,l3_og,l4_og,l5_og,valid_images.shape[0])\n",
        "    acc = compute_classification_acc(classes,valid_labels)\n",
        "    iou = compute_iou(boxes,valid_bboxes)\n",
        "    return acc,iou\n",
        "\n",
        "\n",
        "def evaluate(val_images, model):\n",
        "  print(\"Testing...\")\n",
        "  model.eval()\n",
        "  l1_outputs = []\n",
        "  l2_outputs = []\n",
        "  l3_outputs = []\n",
        "  l4_outputs = []\n",
        "  l5_outputs = []\n",
        "  with torch.no_grad():\n",
        "    index_start = 0\n",
        "    for batch_idx in range(val_images.shape[0]//config['batch_size']):\n",
        "      index_end = min(index_start+config['batch_size'],val_images.shape[0])\n",
        "      images_tensor = torch.tensor(val_images[index_start:index_end],dtype = torch.float32).to(device)\n",
        "      l1,l2,l3,l4,l5 = model(images_tensor.view(-1,1,64,64))\n",
        "      l1_outputs.append(l1)\n",
        "      l2_outputs.append(l2)\n",
        "      l3_outputs.append(l3)\n",
        "      l4_outputs.append(l4)\n",
        "      l5_outputs.append(l5)\n",
        "      index_start = index_end\n",
        "  l1_output = torch.cat(l1_outputs, dim=0)\n",
        "  l2_output = torch.cat(l2_outputs, dim=0)\n",
        "  l3_output = torch.cat(l3_outputs, dim=0)\n",
        "  l4_output = torch.cat(l4_outputs, dim=0)\n",
        "  l5_output = torch.cat(l5_outputs, dim=0)\n",
        "  l1_np = l1_output.cpu().detach().numpy()\n",
        "  l2_np = l2_output.cpu().detach().numpy()\n",
        "  l3_np = l3_output.cpu().detach().numpy()\n",
        "  l4_np = l4_output.cpu().detach().numpy()\n",
        "  l5_np = l5_output.cpu().detach().numpy()\n",
        "  classes = convert_from_labels(l1_np,val_images.shape[0])\n",
        "  l2_og = convert_one_hot_to_original_format(l2_np,val_images.shape[0])\n",
        "  l3_og = convert_one_hot_to_original_format(l3_np,val_images.shape[0])\n",
        "  l4_og = convert_one_hot_to_original_format(l4_np,val_images.shape[0])\n",
        "  l5_og = convert_one_hot_to_original_format(l5_np,val_images.shape[0])\n",
        "  boxes = convert_boxes_to_original_format(l2_og,l3_og,l4_og,l5_og,val_images.shape[0])\n",
        "  return classes,boxes\n",
        "\n",
        "\n",
        "\n",
        "           \n",
        "\n",
        "\n",
        "\n",
        "def convert_to_labels(Y, size):\n",
        "    labels = np.zeros(size, dtype = np.int8)\n",
        "    for i in range(size):\n",
        "      start_index = 0\n",
        "      cur_value = 10\n",
        "      for j in range(Y[i][0]):\n",
        "        start_index += cur_value\n",
        "        cur_value =  cur_value - 1\n",
        "      labels[i] = start_index + Y[i][1] - Y[i][0]\n",
        "    return labels\n",
        "\n",
        "def convert_from_labels(labels, size):\n",
        "    Y = np.zeros((size, 2), dtype=np.int8)\n",
        "    # all possible 2-digit combinations\n",
        "    inx = [[0,0], [0,1], [0,2], [0,3], [0,4], [0,5], [0,6], [0,7], [0,8], [0,9],\n",
        "           [1,1], [1,2], [1,3], [1,4], [1,5], [1,6], [1,7], [1,8], [1,9],\n",
        "           [2,2], [2,3], [2,4], [2,5], [2,6], [2,7], [2,8], [2,9],\n",
        "           [3,3], [3,4], [3,5], [3,6], [3,7], [3,8], [3,9],\n",
        "           [4,4], [4,5], [4,6], [4,7], [4,8], [4,9],\n",
        "           [5,5], [5,6], [5,7], [5,8], [5,9],\n",
        "           [6,6], [6,7], [6,8], [6,9],\n",
        "           [7,7], [7,8], [7,9],\n",
        "           [8,8], [8,8],\n",
        "           [9,9]]\n",
        "    for i in range(size):\n",
        "      Y[i] = inx[np.where(labels[i] == np.amax(labels[i]))[0][0]]\n",
        "    return Y\n",
        "\n",
        "def convert_one_hot_to_original_format(l,size):\n",
        "  temp = np.zeros(size)\n",
        "  j =0\n",
        "  for i in l:\n",
        "    temp[j] = np.where(i == np.amax(i))[0][0]\n",
        "    j+=1\n",
        "  return temp\n",
        "\n",
        "\n",
        "def convert_boxes_to_original_format(l2,l3,l4,l5,size):\n",
        "    pred_bboxes = np.zeros((size,2,4))\n",
        "    pred_bboxes[:,0][:,0] = l2\n",
        "    pred_bboxes[:,0][:,1] = l3\n",
        "    pred_bboxes[:,1][:,0] = l4\n",
        "    pred_bboxes[:,1][:,1] = l5\n",
        "    pred_bboxes[:,0][:,2] = l2 + 28\n",
        "    pred_bboxes[:,0][:,3] = l3 + 28\n",
        "    pred_bboxes[:,1][:,2] = l4 + 28\n",
        "    pred_bboxes[:,1][:,3] = l5 + 28\n",
        "    return np.array(pred_bboxes)\n",
        "\n",
        "def compute_classification_acc(pred, gt):\n",
        "    assert pred.shape == gt.shape\n",
        "    return (pred == gt).astype(int).sum() / gt.size\n",
        "\n",
        "\n",
        "def compute_iou(b_pred, b_gt):\n",
        "    \"\"\"\n",
        "\n",
        "    :param b_pred: predicted bounding boxes, shape=(n,2,4)\n",
        "    :param b_gt: ground truth bounding boxes, shape=(n,2,4)\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    n = np.shape(b_gt)[0]\n",
        "    L_pred = np.zeros((64, 64))\n",
        "    L_gt = np.zeros((64, 64))\n",
        "    iou = 0.0\n",
        "    for i in range(n):\n",
        "        for b in range(2):\n",
        "            rr, cc = polygon([b_pred[i, b, 0], b_pred[i, b, 0], b_pred[i, b, 2], b_pred[i, b, 2]],\n",
        "                             [b_pred[i, b, 1], b_pred[i, b, 3], b_pred[i, b, 3], b_pred[i, b, 1]], [64, 64])\n",
        "            L_pred[rr, cc] = 1\n",
        "\n",
        "            rr, cc = polygon([b_gt[i, b, 0], b_gt[i, b, 0], b_gt[i, b, 2], b_gt[i, b, 2]],\n",
        "                             [b_gt[i, b, 1], b_gt[i, b, 3], b_gt[i, b, 3], b_gt[i, b, 1]], [64, 64])\n",
        "            L_gt[rr, cc] = 1\n",
        "\n",
        "            iou += (1.0 / (2 * n)) * (np.sum((L_pred + L_gt) == 2) / np.sum((L_pred + L_gt) >= 1))\n",
        "\n",
        "            L_pred[:, :] = 0\n",
        "            L_gt[:, :] = 0\n",
        "\n",
        "    return iou\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH80Sc-H8-vF",
        "outputId": "35ee74ca-4816-472f-c54c-87837d84f29d"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from skimage.draw import polygon\n",
        "\n",
        "def compute_classification_acc(pred, gt):\n",
        "    assert pred.shape == gt.shape\n",
        "    return (pred == gt).astype(int).sum() / gt.size\n",
        "\n",
        "\n",
        "def compute_iou(b_pred, b_gt):\n",
        "    \"\"\"\n",
        "\n",
        "    :param b_pred: predicted bounding boxes, shape=(n,2,4)\n",
        "    :param b_gt: ground truth bounding boxes, shape=(n,2,4)\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    n = np.shape(b_gt)[0]\n",
        "    L_pred = np.zeros((64, 64))\n",
        "    L_gt = np.zeros((64, 64))\n",
        "    iou = 0.0\n",
        "    for i in range(n):\n",
        "        for b in range(2):\n",
        "            rr, cc = polygon([b_pred[i, b, 0], b_pred[i, b, 0], b_pred[i, b, 2], b_pred[i, b, 2]],\n",
        "                             [b_pred[i, b, 1], b_pred[i, b, 3], b_pred[i, b, 3], b_pred[i, b, 1]], [64, 64])\n",
        "            L_pred[rr, cc] = 1\n",
        "\n",
        "            rr, cc = polygon([b_gt[i, b, 0], b_gt[i, b, 0], b_gt[i, b, 2], b_gt[i, b, 2]],\n",
        "                             [b_gt[i, b, 1], b_gt[i, b, 3], b_gt[i, b, 3], b_gt[i, b, 1]], [64, 64])\n",
        "            L_gt[rr, cc] = 1\n",
        "\n",
        "            iou += (1.0 / (2 * n)) * (np.sum((L_pred + L_gt) == 2) / np.sum((L_pred + L_gt) >= 1))\n",
        "\n",
        "            L_pred[:, :] = 0\n",
        "            L_gt[:, :] = 0\n",
        "\n",
        "    return iou\n",
        "\n",
        "\n",
        "def main():\n",
        "    # prefix = \"test\"\n",
        "    prefix = \"valid\"\n",
        "    directory = \"/content/gdrive/My Drive/Colab Notebooks/MNISTDD_train_valid/\"\n",
        "    images = np.load(\"/content/gdrive/My Drive/Colab Notebooks/MNISTDD_train_valid/\"+prefix + \"_X.npy\")\n",
        "    start_t = time.time()\n",
        "    pred_class, pred_bboxes = classify_and_detect(images)\n",
        "    end_t = time.time()\n",
        "    gt_class = np.load(\"/content/gdrive/My Drive/Colab Notebooks/MNISTDD_train_valid/\"+prefix + \"_Y.npy\")\n",
        "    gt_bboxes = np.load(\"/content/gdrive/My Drive/Colab Notebooks/MNISTDD_train_valid/\"+prefix + \"_bboxes.npy\")\n",
        "    acc = compute_classification_acc(pred_class, gt_class)\n",
        "    iou = compute_iou(pred_bboxes, gt_bboxes)\n",
        "\n",
        "    time_taken = end_t - start_t\n",
        "\n",
        "    print(f\"Classification Acc: {acc}\")\n",
        "    print(f\"Detection IOU: {iou}\")\n",
        "    print(f\"Test time: {time_taken}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing...\n",
            "Classification Acc: 0.9834\n",
            "Detection IOU: 0.9599756141954755\n",
            "Test time: 25.027446031570435\n"
          ]
        }
      ]
    }
  ]
}